{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DevOps/SRE Documentation Hub \u00b6 Welcome to your streamlined DevOps/SRE documentation system. This hub provides centralized access to planning activities and monitoring resources. \ud83d\udcda Documentation Sections \u00b6 \ud83d\udcc5 Planning \u2b50 Active Content \u00b6 Organized planning documentation with historical records - Overview : Current sprint goals and Q4 2025 objectives - Security Hardening : Comprehensive security improvements (July 2025) - Post-Incident Response : Security improvements following ransomware incident \ud83d\udcca Monitoring Matrix \u2b50 Comprehensive Reference \u00b6 Complete environment-based alert configuration matrix - Detailed alerting rules across all environments (Dev \u2192 Production) - Component-specific monitoring (K8s, databases, networking, applications) - Alert severity levels and routing configurations - Covers bare metal, containers, microservices, and data integrity \ud83c\udfd7\ufe0f Infrastructure \ud83d\udcdd Placeholder \u00b6 Infrastructure documentation and inventory - Currently empty - ready for server inventories and architecture diagrams \ud83d\udcd6 Runbooks \ud83d\udcdd Placeholder \u00b6 Operational procedures and troubleshooting guides - Currently empty - ready for incident response procedures Last Updated: September 10, 2025 Documentation System: MkDocs with Windmill theme","title":"Home"},{"location":"#devopssre-documentation-hub","text":"Welcome to your streamlined DevOps/SRE documentation system. This hub provides centralized access to planning activities and monitoring resources.","title":"DevOps/SRE Documentation Hub"},{"location":"#documentation-sections","text":"","title":"\ud83d\udcda Documentation Sections"},{"location":"#planning-active-content","text":"Organized planning documentation with historical records - Overview : Current sprint goals and Q4 2025 objectives - Security Hardening : Comprehensive security improvements (July 2025) - Post-Incident Response : Security improvements following ransomware incident","title":"\ud83d\udcc5 Planning \u2b50 Active Content"},{"location":"#monitoring-matrix-comprehensive-reference","text":"Complete environment-based alert configuration matrix - Detailed alerting rules across all environments (Dev \u2192 Production) - Component-specific monitoring (K8s, databases, networking, applications) - Alert severity levels and routing configurations - Covers bare metal, containers, microservices, and data integrity","title":"\ud83d\udcca Monitoring Matrix \u2b50 Comprehensive Reference"},{"location":"#infrastructure-placeholder","text":"Infrastructure documentation and inventory - Currently empty - ready for server inventories and architecture diagrams","title":"\ud83c\udfd7\ufe0f Infrastructure \ud83d\udcdd Placeholder"},{"location":"#runbooks-placeholder","text":"Operational procedures and troubleshooting guides - Currently empty - ready for incident response procedures Last Updated: September 10, 2025 Documentation System: MkDocs with Windmill theme","title":"\ud83d\udcd6 Runbooks \ud83d\udcdd Placeholder"},{"location":"infrastructure/","text":"","title":"Infrastructure"},{"location":"monitoring-matrix/","text":"Monitoring & Alerting Matrix \u00b6 Last Updated: September 10, 2025 Environment-based Alert Configuration \u00b6 Alert Component Dev Feature Dev Stable Testing Staging Performance Production Prod Replica Bare Metal / VM CPU \u2705 \u2705 \u2705 \u2705 Memory \u2705 \u2705 \u2705 \u2705 Storage Mesin \u2705 \u2705 \u2705 \u2705 Node Mati \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CPU load per-core usage interrupt rate iowait disk utilization (bytes and inodes) disk latency (await) iops syslog dmesg kernel OOM disk errors Rising iowait with high disk latency filesystem read/write errors CRC/packet drops Unexpected instance termination / reboots / host unreachable K8S Cluster CPU \u2705 \u2705 \u2705 \u2705 Memory \u2705 \u2705 \u2705 \u2705 Klaster Down / Node Ready=false \u2705 \u2705 \u2705 \u2705 \u2705 Block Storage \u2705 \u2705 Pod Crash Pod Down PVC Health Orchestrator 2/2 \u2705 \u2705 \u2705 Kubeconfig will expired APIserver metrics: request latencies (p999/p95), error rates (5xx), authentication errors etcd metrics: leader changes, commit duration, operation latency, database size, disk fsync latency, number of proposals/failed proposals Scheduler metrics: scheduling latency, failed scheduling events, pending pods count Controller-manager: reconcile errors, crashloops Audit logs, apiserver logs, kube-apiserver restarts APIserver latency > 1s for control operations or 5xx errors \u2192 cluster unreliable Etcd leader flapping or high commit latency \u2192 data store instability / risk of data loss Many pods in Pending due to scheduling or volume binding timeouts kubelet health kubelet errors CRI errors container_image_pull_failures kube_pod_container_status_restarts_total oom_kill events Frequent kubelet restarts or container runtime (CRI) errors CrashLoopBackOff spikes / containers restarted many times API & Ingress Layer Request rates 5xx/4xx rates latencies (p50/p95/p99) TLS handshake errors dropped or rate-limited requests Ingress controller logs config errors (invalid routes) Increased 5xx rate error ratio crossing SLO TLS certificate expiring or handshake failures Rate-limit spikes indicating DoS Ceph Cluster OSD Mati OSD Usage Microservices Logs All Microservices Storage Apps Citus \u2705 \u2705 Kafka Redis Microservices \u2705 \u2705 Redis Dapr \u2705 \u2705 Keycloak Patroni Vcluster Static App 3rd party App down DB Down / Crash Redis Down / Crash Keycloak Down / Crash Debezium Down / Crash Kafka Down / Crash Flow CDC Down Minio Down / Crash Storage (PV/PVC/CSI/RBD/Ceph/MinIO) PV capacity PVC usage storage latency (read/write latencies) IO errors \u2705 \u2705 CSI controller logs \u2705 \u2705 Inode usage \u2705 \u2705 \u2705 filesystem remounts/read-only events PVC near-full or PV read-only or FailedAttachVolume High storage latency causing app timeouts Networking CoreDNS latency/errors DNS error rates CNI plugin logs Service endpoints missing iptables or IPVS errors Network packet loss retransmits connection resets socket exhaustion DNS lookup latencies or NXDOMAIN spikes leading to app timeouts Service endpoints mismatch (service has no endpoints) LoadBalancer health check failing for backend pods (traffic blackhole) Nodeport/ingress rules incorrect or failing Keycloak response latency 5xx rate DB errors login failures user lockouts DB connectivity errors Redis / Redis Dapr / Cache memory usage eviction rate ops/sec latency connected clients persistence RDB/AOF errors replication lag role changes eviction spikes (cache thrash) increased latency replication broken persistence failing Kafka broker CPU/io request latency under-replicated partitions (URP) offline partitions consumer group lag partition leader changes URP > 0 controller changes flapping consumer lag growing Citus replication lag CPU active connections slow queries lock waits replication slots disk usage \u2705 checkpoint duration long-running queries high lock contention replication lag connection pool exhaustion Debezium / CDC connector status \u2705 \u2705 \u2705 last processed offset lag error logs connector stopped offset not progressing repeated errors Dapr sidecar health component binding errors actor failures sidecar down for an app MinIO / Object Store disk usage object put/get latency and error rates replication/erasure coding errors access denied spikes mounts read-only object-store errors on PUT/GET Pods & Application Health Readiness & liveness probe failures restarts OOMs CPU throttling thread pool exhaustion request error rates SLO violation rates p95/p99 latencies application logs with errors/exceptions Readiness probe failing (app not receiving traffic) or liveness failing (CrashLoop) App returning 200 but with error payloads (need application-level health checks) High latency or error rate for key transactions even if CPU/memory is fine Data Integrity & Consistency Consumer group lags replication lag read-your-write corrections orphaned locks missing messages audit traces that check consistent flows Replication divergence (Citus shards inconsistent) Silent data loss: message acknowledged but not processed downstream, connectors not committing offsets, partial writes Alert Rules \u00b6 Add your specific alert rules and thresholds here. Alert Routing \u00b6 Define how alerts should be routed based on severity and environment.","title":"Monitoring Matrix"},{"location":"monitoring-matrix/#monitoring-alerting-matrix","text":"Last Updated: September 10, 2025","title":"Monitoring &amp; Alerting Matrix"},{"location":"monitoring-matrix/#environment-based-alert-configuration","text":"Alert Component Dev Feature Dev Stable Testing Staging Performance Production Prod Replica Bare Metal / VM CPU \u2705 \u2705 \u2705 \u2705 Memory \u2705 \u2705 \u2705 \u2705 Storage Mesin \u2705 \u2705 \u2705 \u2705 Node Mati \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CPU load per-core usage interrupt rate iowait disk utilization (bytes and inodes) disk latency (await) iops syslog dmesg kernel OOM disk errors Rising iowait with high disk latency filesystem read/write errors CRC/packet drops Unexpected instance termination / reboots / host unreachable K8S Cluster CPU \u2705 \u2705 \u2705 \u2705 Memory \u2705 \u2705 \u2705 \u2705 Klaster Down / Node Ready=false \u2705 \u2705 \u2705 \u2705 \u2705 Block Storage \u2705 \u2705 Pod Crash Pod Down PVC Health Orchestrator 2/2 \u2705 \u2705 \u2705 Kubeconfig will expired APIserver metrics: request latencies (p999/p95), error rates (5xx), authentication errors etcd metrics: leader changes, commit duration, operation latency, database size, disk fsync latency, number of proposals/failed proposals Scheduler metrics: scheduling latency, failed scheduling events, pending pods count Controller-manager: reconcile errors, crashloops Audit logs, apiserver logs, kube-apiserver restarts APIserver latency > 1s for control operations or 5xx errors \u2192 cluster unreliable Etcd leader flapping or high commit latency \u2192 data store instability / risk of data loss Many pods in Pending due to scheduling or volume binding timeouts kubelet health kubelet errors CRI errors container_image_pull_failures kube_pod_container_status_restarts_total oom_kill events Frequent kubelet restarts or container runtime (CRI) errors CrashLoopBackOff spikes / containers restarted many times API & Ingress Layer Request rates 5xx/4xx rates latencies (p50/p95/p99) TLS handshake errors dropped or rate-limited requests Ingress controller logs config errors (invalid routes) Increased 5xx rate error ratio crossing SLO TLS certificate expiring or handshake failures Rate-limit spikes indicating DoS Ceph Cluster OSD Mati OSD Usage Microservices Logs All Microservices Storage Apps Citus \u2705 \u2705 Kafka Redis Microservices \u2705 \u2705 Redis Dapr \u2705 \u2705 Keycloak Patroni Vcluster Static App 3rd party App down DB Down / Crash Redis Down / Crash Keycloak Down / Crash Debezium Down / Crash Kafka Down / Crash Flow CDC Down Minio Down / Crash Storage (PV/PVC/CSI/RBD/Ceph/MinIO) PV capacity PVC usage storage latency (read/write latencies) IO errors \u2705 \u2705 CSI controller logs \u2705 \u2705 Inode usage \u2705 \u2705 \u2705 filesystem remounts/read-only events PVC near-full or PV read-only or FailedAttachVolume High storage latency causing app timeouts Networking CoreDNS latency/errors DNS error rates CNI plugin logs Service endpoints missing iptables or IPVS errors Network packet loss retransmits connection resets socket exhaustion DNS lookup latencies or NXDOMAIN spikes leading to app timeouts Service endpoints mismatch (service has no endpoints) LoadBalancer health check failing for backend pods (traffic blackhole) Nodeport/ingress rules incorrect or failing Keycloak response latency 5xx rate DB errors login failures user lockouts DB connectivity errors Redis / Redis Dapr / Cache memory usage eviction rate ops/sec latency connected clients persistence RDB/AOF errors replication lag role changes eviction spikes (cache thrash) increased latency replication broken persistence failing Kafka broker CPU/io request latency under-replicated partitions (URP) offline partitions consumer group lag partition leader changes URP > 0 controller changes flapping consumer lag growing Citus replication lag CPU active connections slow queries lock waits replication slots disk usage \u2705 checkpoint duration long-running queries high lock contention replication lag connection pool exhaustion Debezium / CDC connector status \u2705 \u2705 \u2705 last processed offset lag error logs connector stopped offset not progressing repeated errors Dapr sidecar health component binding errors actor failures sidecar down for an app MinIO / Object Store disk usage object put/get latency and error rates replication/erasure coding errors access denied spikes mounts read-only object-store errors on PUT/GET Pods & Application Health Readiness & liveness probe failures restarts OOMs CPU throttling thread pool exhaustion request error rates SLO violation rates p95/p99 latencies application logs with errors/exceptions Readiness probe failing (app not receiving traffic) or liveness failing (CrashLoop) App returning 200 but with error payloads (need application-level health checks) High latency or error rate for key transactions even if CPU/memory is fine Data Integrity & Consistency Consumer group lags replication lag read-your-write corrections orphaned locks missing messages audit traces that check consistent flows Replication divergence (Citus shards inconsistent) Silent data loss: message acknowledged but not processed downstream, connectors not committing offsets, partial writes","title":"Environment-based Alert Configuration"},{"location":"monitoring-matrix/#alert-rules","text":"Add your specific alert rules and thresholds here.","title":"Alert Rules"},{"location":"monitoring-matrix/#alert-routing","text":"Define how alerts should be routed based on severity and environment.","title":"Alert Routing"},{"location":"runbooks/","text":"","title":"Runbooks"},{"location":"planning/","text":"\ud83d\udcda Planning Documentation \u00b6 Security Hardening Project - Comprehensive security improvements (July 2025) Post-Incident Response (Ransomware) - Security improvements following ransomware incident Ceph Memory Limit Fix - Fix memory limit on OSD in Ceph cluster Monitoring Kubernetes Phase 1 - Comprehensive Kubernetes monitoring implementation Monitoring Kubernetes Phase 2 - Storage monitoring and sidecar implementation","title":"Overview"},{"location":"planning/#planning-documentation","text":"Security Hardening Project - Comprehensive security improvements (July 2025) Post-Incident Response (Ransomware) - Security improvements following ransomware incident Ceph Memory Limit Fix - Fix memory limit on OSD in Ceph cluster Monitoring Kubernetes Phase 1 - Comprehensive Kubernetes monitoring implementation Monitoring Kubernetes Phase 2 - Storage monitoring and sidecar implementation","title":"\ud83d\udcda Planning Documentation"},{"location":"planning/ceph-memory-limit-fix/","text":"Fix Memory Limit on OSD in Ceph Cluster \u00b6 Project Overview \u00b6 This planning document outlines the project to fix memory limit issues on OSDs (Object Storage Daemons) in our Ceph cluster. The project follows a phased approach starting with POC in a replica environment before implementing in production. Project Tasks \u00b6 Phase Task SubTasks Estimate Hours Goal Status Phase 1: Environment Preparation Create replica k8s cluster production in colo - Create new VMs - Install k0s, vcluster, rook ceph cluster, DB Citus - Match configuration with production 6 POC before production implementation Done Phase 2: POC Implementation Execute safe POC fix memory limit - Test memory limit adjustments - Monitor cluster stability - Document findings 4 Validate fix approach safely Done Phase 3: Backup & Recovery Prepare full backup mechanism for performance & production - Setup backup procedures - Test backup integrity - Document backup process - Backup plan if issues occur in production Done Phase 3: Backup & Recovery Prepare full restore mechanism for performance & production - Setup restore procedures - Test restore process - Document restore steps - Recovery plan if issues occur in production Done Phase 4: Production Implementation Implement fix memory limit osd ceph in production - Apply memory limit configurations - Monitor cluster health - Verify performance improvements - Resolve production memory issues Done Project Summary \u00b6 Total Estimated Hours: 10+ hours Key Achievements: - \u2705 Successfully created replica environment for safe testing - \u2705 Validated memory limit fix approach through POC - \u2705 Established comprehensive backup and restore procedures - \u2705 Successfully implemented fix in production environment Impact: Resolved Ceph OSD memory limit issues, improving cluster stability and performance. Priority: High - Critical infrastructure improvement Lessons Learned \u00b6 POC Approach: Testing in replica environment prevented production risks Backup Strategy: Having comprehensive backup/restore procedures provided confidence for production changes Phased Implementation: Step-by-step approach ensured successful deployment Last Updated: September 10, 2025","title":"Ceph Memory Limit Fix"},{"location":"planning/ceph-memory-limit-fix/#fix-memory-limit-on-osd-in-ceph-cluster","text":"","title":"Fix Memory Limit on OSD in Ceph Cluster"},{"location":"planning/ceph-memory-limit-fix/#project-overview","text":"This planning document outlines the project to fix memory limit issues on OSDs (Object Storage Daemons) in our Ceph cluster. The project follows a phased approach starting with POC in a replica environment before implementing in production.","title":"Project Overview"},{"location":"planning/ceph-memory-limit-fix/#project-tasks","text":"Phase Task SubTasks Estimate Hours Goal Status Phase 1: Environment Preparation Create replica k8s cluster production in colo - Create new VMs - Install k0s, vcluster, rook ceph cluster, DB Citus - Match configuration with production 6 POC before production implementation Done Phase 2: POC Implementation Execute safe POC fix memory limit - Test memory limit adjustments - Monitor cluster stability - Document findings 4 Validate fix approach safely Done Phase 3: Backup & Recovery Prepare full backup mechanism for performance & production - Setup backup procedures - Test backup integrity - Document backup process - Backup plan if issues occur in production Done Phase 3: Backup & Recovery Prepare full restore mechanism for performance & production - Setup restore procedures - Test restore process - Document restore steps - Recovery plan if issues occur in production Done Phase 4: Production Implementation Implement fix memory limit osd ceph in production - Apply memory limit configurations - Monitor cluster health - Verify performance improvements - Resolve production memory issues Done","title":"Project Tasks"},{"location":"planning/ceph-memory-limit-fix/#project-summary","text":"Total Estimated Hours: 10+ hours Key Achievements: - \u2705 Successfully created replica environment for safe testing - \u2705 Validated memory limit fix approach through POC - \u2705 Established comprehensive backup and restore procedures - \u2705 Successfully implemented fix in production environment Impact: Resolved Ceph OSD memory limit issues, improving cluster stability and performance. Priority: High - Critical infrastructure improvement","title":"Project Summary"},{"location":"planning/ceph-memory-limit-fix/#lessons-learned","text":"POC Approach: Testing in replica environment prevented production risks Backup Strategy: Having comprehensive backup/restore procedures provided confidence for production changes Phased Implementation: Step-by-step approach ensured successful deployment Last Updated: September 10, 2025","title":"Lessons Learned"},{"location":"planning/monitoring-kubernetes-phase2/","text":"Monitoring Kubernetes Phase 2 \u00b6 Project Overview \u00b6 This planning document outlines Phase 2 of the Kubernetes monitoring implementation project. This phase focuses on storage monitoring with Ceph cluster observability and stateful application monitoring through sidecar containers. Project Tasks \u00b6 Task SubTasks Estimate Hours Priority Status Create monitoring for Ceph cluster ceph_health_status, osd_up, ceph_cluster_total_used_bytes 4 High Done Create monitoring for Ceph pool ceph_pool_stored, ceph_pool_max_avail, alert if usage > 70% 3 High Done Create monitoring stateful with sidecar POC Add sidecar Prometheus exporter (df, du, iostat) for volume usage per PVC, inject to stateful pods like PostgreSQL, Kafka, etc. 3 Medium Done Add sidecar monitoring & alert volume citus testing Configure volume monitoring for Citus testing environment 1 Medium Done Add sidecar monitoring & alert volume keycloak testing Configure volume monitoring for Keycloak testing environment 1 Medium In Progress Add sidecar monitoring & alert volume redis testing Configure volume monitoring for Redis testing environment 1 Medium Done Add sidecar monitoring & alert volume kafka testing Configure volume monitoring for Kafka testing environment 1 Medium In Progress Add sidecar monitoring & alert volume citus staging Configure volume monitoring for Citus staging environment 1 Medium Done Add sidecar monitoring & alert volume keycloak staging Configure volume monitoring for Keycloak staging environment 1 Medium In Progress Add sidecar monitoring & alert volume redis staging Configure volume monitoring for Redis staging environment 1 Medium Done Add sidecar monitoring & alert volume kafka staging Configure volume monitoring for Kafka staging environment 1 Medium In Progress Project Summary \u00b6 Total Estimated Hours: 18+ hours Progress Status: - \u2705 Ceph cluster monitoring - Health status, OSD status, storage utilization - \u2705 Ceph pool monitoring - Pool storage metrics with 70% usage alerts - \u2705 Sidecar POC - Volume monitoring framework implementation - \u2705 Testing environment - Citus and Redis volume monitoring - \ud83d\udd04 Testing environment - Keycloak and Kafka volume monitoring - \u2705 Staging environment - Citus and Redis volume monitoring - \ud83d\udd04 Staging environment - Keycloak and Kafka volume monitoring Key Achievements: - Established comprehensive Ceph storage monitoring - Implemented automated pool usage alerting at 70% threshold - Successfully deployed sidecar monitoring framework - Deployed volume monitoring for critical stateful services Impact: Enhanced storage observability and proactive capacity management for Kubernetes workloads. Priority: High - Critical storage infrastructure monitoring Last Updated: September 10, 2025","title":"Monitoring Kubernetes Phase 2"},{"location":"planning/monitoring-kubernetes-phase2/#monitoring-kubernetes-phase-2","text":"","title":"Monitoring Kubernetes Phase 2"},{"location":"planning/monitoring-kubernetes-phase2/#project-overview","text":"This planning document outlines Phase 2 of the Kubernetes monitoring implementation project. This phase focuses on storage monitoring with Ceph cluster observability and stateful application monitoring through sidecar containers.","title":"Project Overview"},{"location":"planning/monitoring-kubernetes-phase2/#project-tasks","text":"Task SubTasks Estimate Hours Priority Status Create monitoring for Ceph cluster ceph_health_status, osd_up, ceph_cluster_total_used_bytes 4 High Done Create monitoring for Ceph pool ceph_pool_stored, ceph_pool_max_avail, alert if usage > 70% 3 High Done Create monitoring stateful with sidecar POC Add sidecar Prometheus exporter (df, du, iostat) for volume usage per PVC, inject to stateful pods like PostgreSQL, Kafka, etc. 3 Medium Done Add sidecar monitoring & alert volume citus testing Configure volume monitoring for Citus testing environment 1 Medium Done Add sidecar monitoring & alert volume keycloak testing Configure volume monitoring for Keycloak testing environment 1 Medium In Progress Add sidecar monitoring & alert volume redis testing Configure volume monitoring for Redis testing environment 1 Medium Done Add sidecar monitoring & alert volume kafka testing Configure volume monitoring for Kafka testing environment 1 Medium In Progress Add sidecar monitoring & alert volume citus staging Configure volume monitoring for Citus staging environment 1 Medium Done Add sidecar monitoring & alert volume keycloak staging Configure volume monitoring for Keycloak staging environment 1 Medium In Progress Add sidecar monitoring & alert volume redis staging Configure volume monitoring for Redis staging environment 1 Medium Done Add sidecar monitoring & alert volume kafka staging Configure volume monitoring for Kafka staging environment 1 Medium In Progress","title":"Project Tasks"},{"location":"planning/monitoring-kubernetes-phase2/#project-summary","text":"Total Estimated Hours: 18+ hours Progress Status: - \u2705 Ceph cluster monitoring - Health status, OSD status, storage utilization - \u2705 Ceph pool monitoring - Pool storage metrics with 70% usage alerts - \u2705 Sidecar POC - Volume monitoring framework implementation - \u2705 Testing environment - Citus and Redis volume monitoring - \ud83d\udd04 Testing environment - Keycloak and Kafka volume monitoring - \u2705 Staging environment - Citus and Redis volume monitoring - \ud83d\udd04 Staging environment - Keycloak and Kafka volume monitoring Key Achievements: - Established comprehensive Ceph storage monitoring - Implemented automated pool usage alerting at 70% threshold - Successfully deployed sidecar monitoring framework - Deployed volume monitoring for critical stateful services Impact: Enhanced storage observability and proactive capacity management for Kubernetes workloads. Priority: High - Critical storage infrastructure monitoring Last Updated: September 10, 2025","title":"Project Summary"},{"location":"planning/monitoring-kubernetes/","text":"Monitoring Kubernetes Phase 1 \u00b6 Project Overview \u00b6 This planning document outlines Phase 1 of the Kubernetes monitoring implementation project. The project focuses on establishing comprehensive monitoring coverage for all critical Kubernetes components and integrating alerts with our communication platform. Project Tasks \u00b6 Task SubTasks Estimate Hours Goal Status Create monitoring for Nodes CPU, Memory, Disk usage, Pressure, Network, Ready status 4 Monitor node health and resource utilization Done Create monitoring for Kubelet / Kube-Proxy / Containerd Process health, errors, resource usage 3 Monitor system component health Done Create monitoring for Control Plane (API Server, Scheduler, Controller Manager) Latency, error rate, leader election, request queue, etc. 3 Monitor control plane performance and availability Done Create monitoring for Etcd Disk I/O, latency, compaction, DB size, quorum 4 Monitor etcd cluster health and performance Done Create monitoring for Pod & Container Lifecycle Monitor CrashLoopBackOff, OOMKills, image pull errors 3 Monitor application workload health Done Integrate monitoring to Zulip Configure alert routing and notification formatting 3 Enable real-time incident notifications Done Project Summary \u00b6 Total Estimated Hours: 20 hours Progress Status: - \u2705 Node monitoring - Comprehensive resource and health tracking - \u2705 System components - Kubelet, Kube-Proxy, Containerd monitoring - \u2705 Control plane - API Server, Scheduler, Controller Manager monitoring - \ud83d\udd04 Etcd monitoring - Database performance and cluster health tracking - \u2705 Pod lifecycle - Container crash and resource monitoring - \u2705 Zulip integration - Real-time alert notifications Key Achievements: - Established baseline monitoring for Kubernetes infrastructure - Implemented workload health monitoring with crash detection - Successfully integrated alerts with Zulip for immediate notifications - Created comprehensive control plane observability Impact: Enhanced Kubernetes cluster observability, enabling proactive issue detection and faster incident response. Priority: High - Critical infrastructure monitoring Last Updated: September 10, 2025","title":"Monitoring Kubernetes Phase 1"},{"location":"planning/monitoring-kubernetes/#monitoring-kubernetes-phase-1","text":"","title":"Monitoring Kubernetes Phase 1"},{"location":"planning/monitoring-kubernetes/#project-overview","text":"This planning document outlines Phase 1 of the Kubernetes monitoring implementation project. The project focuses on establishing comprehensive monitoring coverage for all critical Kubernetes components and integrating alerts with our communication platform.","title":"Project Overview"},{"location":"planning/monitoring-kubernetes/#project-tasks","text":"Task SubTasks Estimate Hours Goal Status Create monitoring for Nodes CPU, Memory, Disk usage, Pressure, Network, Ready status 4 Monitor node health and resource utilization Done Create monitoring for Kubelet / Kube-Proxy / Containerd Process health, errors, resource usage 3 Monitor system component health Done Create monitoring for Control Plane (API Server, Scheduler, Controller Manager) Latency, error rate, leader election, request queue, etc. 3 Monitor control plane performance and availability Done Create monitoring for Etcd Disk I/O, latency, compaction, DB size, quorum 4 Monitor etcd cluster health and performance Done Create monitoring for Pod & Container Lifecycle Monitor CrashLoopBackOff, OOMKills, image pull errors 3 Monitor application workload health Done Integrate monitoring to Zulip Configure alert routing and notification formatting 3 Enable real-time incident notifications Done","title":"Project Tasks"},{"location":"planning/monitoring-kubernetes/#project-summary","text":"Total Estimated Hours: 20 hours Progress Status: - \u2705 Node monitoring - Comprehensive resource and health tracking - \u2705 System components - Kubelet, Kube-Proxy, Containerd monitoring - \u2705 Control plane - API Server, Scheduler, Controller Manager monitoring - \ud83d\udd04 Etcd monitoring - Database performance and cluster health tracking - \u2705 Pod lifecycle - Container crash and resource monitoring - \u2705 Zulip integration - Real-time alert notifications Key Achievements: - Established baseline monitoring for Kubernetes infrastructure - Implemented workload health monitoring with crash detection - Successfully integrated alerts with Zulip for immediate notifications - Created comprehensive control plane observability Impact: Enhanced Kubernetes cluster observability, enabling proactive issue detection and faster incident response. Priority: High - Critical infrastructure monitoring Last Updated: September 10, 2025","title":"Project Summary"},{"location":"planning/post-incident/","text":"Post-Incident Response & Security Improvements \u00b6 Phase Task SubTask Estimate Hour Goal Status 0 Mengamankan iDRAC Mengganti semua password dari dokumen lama 1 Menghilangkan potensi akses tidak sah melalui kredensial lama Done Membuat username devops baru 1 Done Menonaktifkan/Menghapus akses root dan user lain 1 Memisahkan akses administratif dengan kredensial unik Done Periksa Kunci SSH yang tersimpan 2 Menghapus akses berbasis SSH yang tidak sah Done Cek Integrasi Direktori (LDAP/Active Directory) 2 Memastikan integrasi sesuai kebijakan keamanan Done Audit Log 2 Mengidentifikasi aktivitas mencurigakan Done Implementasi Remote Syslog 4 Memastikan semua log terkirim ke server terpusat Cancelled Tracing IP mencurigakan 192.168.80.1 & 192.168.24.1 saat kejadian ransomware Capture log lalu lintas jaringan pada waktu kejadian 4 Mendapatkan bukti lalu lintas terkait serangan Done Identifikasi source & destination connection mencurigakan 3 Menentukan titik masuk & arah serangan Cancelled 1 Scanning RDP Windows Baru di Colo Membuat RDP baru di Colo 1 Cancelled Jalankan port scan untuk deteksi RDP terbuka 1 Cancelled Periksa konfigurasi keamanan RDP (NLA, whitelist IP) 2 Memastikan RDP aman Cancelled 2 Scanning RDP salah satu Customer di Colo 6 Cancelled 3 POC MikroTik di Blok C Deploy MikroTik lab dengan topologi VLAN sama persis (24, 25, 99, dll) 4 Replikasi environment Colo untuk testing Done Terapkan aturan masquerade yang sama seperti konfigurasi Colo (baseline) 2 Memastikan hasil uji identik dengan kondisi existing Done Uji logging saat serangan internal (ssh brute-force simulasi) 2 Validasi problem log visibility Done Terapkan perbaikan (hapus masquerade antar VLAN, segmentasi firewall, mgmt ACL) 4 Membuktikan solusi bisa jalan tanpa ganggu traffic Done Dokumentasikan hasil test (before/after config & log visibility) 2 Jadi referensi implementasi di Colo Cancelled 4 Perbaikan Konfigurasi MikroTik Colo Hapus aturan masquerade antar VLAN di /ip firewall nat 2 Pastikan IP source tercatat dengan benar di log Cancelled Terapkan aturan firewall antar VLAN (deny by default, allow by need) 3 Segmentasi lalu lintas internal Cancelled Audit NAT dan ACL lainnya 2 Pastikan tidak ada celah eskalasi Done 5 Deploy Visibility & Monitoring Implementasi remote syslog ke server SIEM/ELK 6 Centralized log untuk analisis Done Deploy EDR ringan (misal Wazuh/Velociraptor) di semua VM 3 Deteksi serangan endpoint real-time Done 6 Implementasi Kontrol Akses Deploy PAM untuk akun administratif 3 Kredensial administratif terkontrol Planned Last Updated: September 10, 2025","title":"Post-Incident Response (Ransomware)"},{"location":"planning/post-incident/#post-incident-response-security-improvements","text":"Phase Task SubTask Estimate Hour Goal Status 0 Mengamankan iDRAC Mengganti semua password dari dokumen lama 1 Menghilangkan potensi akses tidak sah melalui kredensial lama Done Membuat username devops baru 1 Done Menonaktifkan/Menghapus akses root dan user lain 1 Memisahkan akses administratif dengan kredensial unik Done Periksa Kunci SSH yang tersimpan 2 Menghapus akses berbasis SSH yang tidak sah Done Cek Integrasi Direktori (LDAP/Active Directory) 2 Memastikan integrasi sesuai kebijakan keamanan Done Audit Log 2 Mengidentifikasi aktivitas mencurigakan Done Implementasi Remote Syslog 4 Memastikan semua log terkirim ke server terpusat Cancelled Tracing IP mencurigakan 192.168.80.1 & 192.168.24.1 saat kejadian ransomware Capture log lalu lintas jaringan pada waktu kejadian 4 Mendapatkan bukti lalu lintas terkait serangan Done Identifikasi source & destination connection mencurigakan 3 Menentukan titik masuk & arah serangan Cancelled 1 Scanning RDP Windows Baru di Colo Membuat RDP baru di Colo 1 Cancelled Jalankan port scan untuk deteksi RDP terbuka 1 Cancelled Periksa konfigurasi keamanan RDP (NLA, whitelist IP) 2 Memastikan RDP aman Cancelled 2 Scanning RDP salah satu Customer di Colo 6 Cancelled 3 POC MikroTik di Blok C Deploy MikroTik lab dengan topologi VLAN sama persis (24, 25, 99, dll) 4 Replikasi environment Colo untuk testing Done Terapkan aturan masquerade yang sama seperti konfigurasi Colo (baseline) 2 Memastikan hasil uji identik dengan kondisi existing Done Uji logging saat serangan internal (ssh brute-force simulasi) 2 Validasi problem log visibility Done Terapkan perbaikan (hapus masquerade antar VLAN, segmentasi firewall, mgmt ACL) 4 Membuktikan solusi bisa jalan tanpa ganggu traffic Done Dokumentasikan hasil test (before/after config & log visibility) 2 Jadi referensi implementasi di Colo Cancelled 4 Perbaikan Konfigurasi MikroTik Colo Hapus aturan masquerade antar VLAN di /ip firewall nat 2 Pastikan IP source tercatat dengan benar di log Cancelled Terapkan aturan firewall antar VLAN (deny by default, allow by need) 3 Segmentasi lalu lintas internal Cancelled Audit NAT dan ACL lainnya 2 Pastikan tidak ada celah eskalasi Done 5 Deploy Visibility & Monitoring Implementasi remote syslog ke server SIEM/ELK 6 Centralized log untuk analisis Done Deploy EDR ringan (misal Wazuh/Velociraptor) di semua VM 3 Deteksi serangan endpoint real-time Done 6 Implementasi Kontrol Akses Deploy PAM untuk akun administratif 3 Kredensial administratif terkontrol Planned Last Updated: September 10, 2025","title":"Post-Incident Response &amp; Security Improvements"},{"location":"planning/security-hardening/","text":"Security Hardening Project (July 2025) \u00b6 Phase Task SubTasks Estimate Hour Goal Impact Priority Status Report Start Date End Date 0 - Credential & Asset Data Collection Inventory All Credentials Mikrotik, Bare Metal, VM, RDP, Portal 6 Map all current accounts and reduce unknowns High Done List Password Andal 23 Juni 2025.xlsx - - Collect Stored Keys & Secrets Audit [ssh] /authorized_keys on all servers Check credential storage systems Identify hardcoded credentials 6 Identify reused, weak, or stale credentials High Done Report - Collect Stored Keys & Secrets 2 Juli 2025 2 Juli 2025 Centralize & Encrypt Collected Data Store findings in encrypted vault Tag each with rotation status 4 Secure sensitive info while enabling analysis High Cancelled Sudah implementasi ZTNA - - Identify Orphaned & Unused Accounts Look for last login timestamps Disable accounts unused for 90+ days 4 Reduce attack surface by removing dormant accounts Someone may lose access Medium Done Last Login Timestamps VM Colo 3 Juli 2025 3 Juli 2025 1 - Evidence Preservation Preserve Evidence Connect to MikroTik Run /export file=backup.rsc 3 Retain configuration and system evidence High Done Report - Preserve Evidence Mikrotik2 4 Juli 2025 4 Juli 2025 2 - Compromise & Backdoor Scanning Scan for known Linux rootkits & backdoors Run chkrootkit, rkhunter, YARA, etc. 8 Done velociraptor - scan_results 4 Juli 2025 7 Juli 2025 Inspect unusual system binaries Hash critical binaries and compare to known-good 6 Done velociraptor - analzye 7 Juli 2025 8 Juli 2025 Audit crontabs, rc.local, systemd, startup scripts Look for unauthorized jobs or persistence 6 Done 7 Juli 2025 8 Juli 2025 Network backdoor check Use ss, netstat, lsof, Wireshark/tcpdump, Nmap scans 6 Done network_backdoor_check - OneDrive 9 Juli 2025 11 Juli 2025 Review backdoor scan results 16 14 Juli 2025 14 Juli 2025 3 - Secure Rebuild & Hardening Mikrotik Service Minimization Disable telnet, ftp, www, api, api-ssl Allow Winbox/SSH only from jump host 4 Reduce attack surface Loss of legacy service access High Done 15 Juli 2025 15 Juli 2025 Mikrotik Firewall Hardening First rule: drop invalid connections Explicit allow rules only Block remote mgmt from public 8 Control ingress/egress precisely Risk of lockout or service drop High Done 15 Juli 2025 15 Juli 2025 Mikrotik SSH Security Enforce SSH key auth Disable root login Enable strong crypto 6 Prevent brute-force & privilege escalation SSH with password not available High Done 16 Juli 2025 16 Juli 2025 Mikrotik Secure Logging Configure remote syslog to Wazuh/Logstash 8 Enable centralized visibility High Done 17 Juli 2025 17 Juli 2025 Infrastructure Fresh Reinstall Reimage all systems from known-good ISOs 16 Eliminate rootkits/backdoors Downtime High Cancelled belum memungkinkan melakukan karena kurang dokumentasi 10 Juli 2025 Infrastructure Patch Management Register all systems to update service 8 Prevent exploits via known CVEs High Planned 14 Juli 2025 VLAN Segmentation Define VLANs: mgmt, web, app, db, office 16 Limit lateral movement Communication breaks likely High Cancelled berisiko network breaks 15 Juli 2025 WireGuard Full Mesh VPN Deploy WireGuard Server in internal cluster 16 Secure site-to-site transport Current connections might be disturbed High Cancelled https://teleport-cluster.test.andalsoftware.com/ 17 Juli 2025 ZTNA for Remote Access Deploy Cloudflare Access or Tailscale 8 No network exposure; app-level trust High Cancelled sudah implementasi ZTNA teleport 18 Juli 2025 4 - Monitoring, IAM, and Operations Deploy Wazuh SIEM Install Wazuh Indexer, server and dashboard Upgrade version Set up SSL certificates 24 Full visibility and alerting High Done https://wazuh.andalsoftware.com 3 Juli 2025 7 Juli 2025 MikroTik Log Forwarding /system logging to send logs to Logstash 6 Central log analysis High Done https://kibana.andalsoftware.com 17 Juli 2025 17 Juli 2025 Alerting Rules Setup Failed logins, abnormal geolocation FIM triggers, privilege escalation 8 Timely breach detection High Planned 23 Juli 2025 Deploy FreeIPA Cluster Install HA FreeIPA Harden root/admin access 16 Unified user directory High Cancelled sudah implementasi ZTNA teleport 25 Juli 2025 FreeIPA Group Policies Define groups by role HBAC access rules 8 Role-based access control Restriction enforcement High Cancelled sudah implementasi ZTNA teleport 29 Juli 2025 MikroTik RADIUS Integration Set up FreeRADIUS Configure /radius login 8 Central login and auditing High Cancelled 30 Juli 2025 SSH Hardening Across Servers SSSD + FreeIPA + MFA Disable password SSH logins 16 Central key control & MFA enforcement High Cancelled akses SSH sudah disable semua ke mikrotik 31 Juli 2025 Last Updated: September 10, 2025","title":"Security Hardening"},{"location":"planning/security-hardening/#security-hardening-project-july-2025","text":"Phase Task SubTasks Estimate Hour Goal Impact Priority Status Report Start Date End Date 0 - Credential & Asset Data Collection Inventory All Credentials Mikrotik, Bare Metal, VM, RDP, Portal 6 Map all current accounts and reduce unknowns High Done List Password Andal 23 Juni 2025.xlsx - - Collect Stored Keys & Secrets Audit [ssh] /authorized_keys on all servers Check credential storage systems Identify hardcoded credentials 6 Identify reused, weak, or stale credentials High Done Report - Collect Stored Keys & Secrets 2 Juli 2025 2 Juli 2025 Centralize & Encrypt Collected Data Store findings in encrypted vault Tag each with rotation status 4 Secure sensitive info while enabling analysis High Cancelled Sudah implementasi ZTNA - - Identify Orphaned & Unused Accounts Look for last login timestamps Disable accounts unused for 90+ days 4 Reduce attack surface by removing dormant accounts Someone may lose access Medium Done Last Login Timestamps VM Colo 3 Juli 2025 3 Juli 2025 1 - Evidence Preservation Preserve Evidence Connect to MikroTik Run /export file=backup.rsc 3 Retain configuration and system evidence High Done Report - Preserve Evidence Mikrotik2 4 Juli 2025 4 Juli 2025 2 - Compromise & Backdoor Scanning Scan for known Linux rootkits & backdoors Run chkrootkit, rkhunter, YARA, etc. 8 Done velociraptor - scan_results 4 Juli 2025 7 Juli 2025 Inspect unusual system binaries Hash critical binaries and compare to known-good 6 Done velociraptor - analzye 7 Juli 2025 8 Juli 2025 Audit crontabs, rc.local, systemd, startup scripts Look for unauthorized jobs or persistence 6 Done 7 Juli 2025 8 Juli 2025 Network backdoor check Use ss, netstat, lsof, Wireshark/tcpdump, Nmap scans 6 Done network_backdoor_check - OneDrive 9 Juli 2025 11 Juli 2025 Review backdoor scan results 16 14 Juli 2025 14 Juli 2025 3 - Secure Rebuild & Hardening Mikrotik Service Minimization Disable telnet, ftp, www, api, api-ssl Allow Winbox/SSH only from jump host 4 Reduce attack surface Loss of legacy service access High Done 15 Juli 2025 15 Juli 2025 Mikrotik Firewall Hardening First rule: drop invalid connections Explicit allow rules only Block remote mgmt from public 8 Control ingress/egress precisely Risk of lockout or service drop High Done 15 Juli 2025 15 Juli 2025 Mikrotik SSH Security Enforce SSH key auth Disable root login Enable strong crypto 6 Prevent brute-force & privilege escalation SSH with password not available High Done 16 Juli 2025 16 Juli 2025 Mikrotik Secure Logging Configure remote syslog to Wazuh/Logstash 8 Enable centralized visibility High Done 17 Juli 2025 17 Juli 2025 Infrastructure Fresh Reinstall Reimage all systems from known-good ISOs 16 Eliminate rootkits/backdoors Downtime High Cancelled belum memungkinkan melakukan karena kurang dokumentasi 10 Juli 2025 Infrastructure Patch Management Register all systems to update service 8 Prevent exploits via known CVEs High Planned 14 Juli 2025 VLAN Segmentation Define VLANs: mgmt, web, app, db, office 16 Limit lateral movement Communication breaks likely High Cancelled berisiko network breaks 15 Juli 2025 WireGuard Full Mesh VPN Deploy WireGuard Server in internal cluster 16 Secure site-to-site transport Current connections might be disturbed High Cancelled https://teleport-cluster.test.andalsoftware.com/ 17 Juli 2025 ZTNA for Remote Access Deploy Cloudflare Access or Tailscale 8 No network exposure; app-level trust High Cancelled sudah implementasi ZTNA teleport 18 Juli 2025 4 - Monitoring, IAM, and Operations Deploy Wazuh SIEM Install Wazuh Indexer, server and dashboard Upgrade version Set up SSL certificates 24 Full visibility and alerting High Done https://wazuh.andalsoftware.com 3 Juli 2025 7 Juli 2025 MikroTik Log Forwarding /system logging to send logs to Logstash 6 Central log analysis High Done https://kibana.andalsoftware.com 17 Juli 2025 17 Juli 2025 Alerting Rules Setup Failed logins, abnormal geolocation FIM triggers, privilege escalation 8 Timely breach detection High Planned 23 Juli 2025 Deploy FreeIPA Cluster Install HA FreeIPA Harden root/admin access 16 Unified user directory High Cancelled sudah implementasi ZTNA teleport 25 Juli 2025 FreeIPA Group Policies Define groups by role HBAC access rules 8 Role-based access control Restriction enforcement High Cancelled sudah implementasi ZTNA teleport 29 Juli 2025 MikroTik RADIUS Integration Set up FreeRADIUS Configure /radius login 8 Central login and auditing High Cancelled 30 Juli 2025 SSH Hardening Across Servers SSSD + FreeIPA + MFA Disable password SSH logins 16 Central key control & MFA enforcement High Cancelled akses SSH sudah disable semua ke mikrotik 31 Juli 2025 Last Updated: September 10, 2025","title":"Security Hardening Project (July 2025)"}]}